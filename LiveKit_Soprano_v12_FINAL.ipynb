{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¤ LiveKit Voice Agent v12 - FINAL\n",
                "\n",
                "**Local Whisper STT â†’ Gemini LLM â†’ Soprano TTS**\n",
                "\n",
                "Fixed: Uses google.LLM (works with API key) + custom STT/TTS nodes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q \"livekit-agents[google,silero]~=1.3\" soprano-tts faster-whisper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# LiveKit credentials\n",
                "os.environ[\"LIVEKIT_URL\"] = \"wss://test-jllkasbg.livekit.cloud\"\n",
                "os.environ[\"LIVEKIT_API_KEY\"] = \"APIFnsAaWh3eFdR\"\n",
                "os.environ[\"LIVEKIT_API_SECRET\"] = \"WabCvkbupgaGfV7JQKBdZNDlYXuRFrr9jZcu7HTFdfG\"\n",
                "\n",
                "# Gemini API Key (works for google.LLM!)\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD9sGx9FmvzIl7NtgU7vdwJVgs7NohSSqI\"\n",
                "\n",
                "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
                "\n",
                "print(\"âœ… Credentials set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile agent_v12.py\n",
                "\"\"\"LiveKit Voice Agent v12: Local Whisper + google.LLM + Soprano TTS\"\"\"\n",
                "\n",
                "import asyncio\n",
                "import os\n",
                "import re\n",
                "import numpy as np\n",
                "from typing import AsyncIterable\n",
                "\n",
                "from livekit import agents, rtc\n",
                "from livekit.agents import Agent, AgentSession, ModelSettings, cli, stt\n",
                "from livekit.plugins import google, silero\n",
                "\n",
                "# Global models\n",
                "SOPRANO = None\n",
                "WHISPER = None\n",
                "\n",
                "\n",
                "def load_models():\n",
                "    global SOPRANO, WHISPER\n",
                "    \n",
                "    if WHISPER is None:\n",
                "        print(\"Loading Faster Whisper...\")\n",
                "        from faster_whisper import WhisperModel\n",
                "        WHISPER = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
                "        print(\"âœ… Whisper ready\")\n",
                "    \n",
                "    if SOPRANO is None:\n",
                "        print(\"Loading Soprano TTS...\")\n",
                "        from soprano import SopranoTTS\n",
                "        SOPRANO = SopranoTTS(device=\"cuda\")\n",
                "        print(\"âœ… Soprano TTS ready\")\n",
                "\n",
                "\n",
                "class WhisperSTT(stt.STT):\n",
                "    \"\"\"Custom STT using local Faster Whisper\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        super().__init__(\n",
                "            capabilities=stt.STTCapabilities(streaming=False, interim_results=False)\n",
                "        )\n",
                "    \n",
                "    async def _recognize_impl(\n",
                "        self,\n",
                "        buffer: utils.AudioBuffer,\n",
                "        *,\n",
                "        language: str | None = None,\n",
                "        conn_options = None,\n",
                "    ) -> stt.SpeechEvent:\n",
                "        # Get audio data\n",
                "        frame = buffer.to_frame()\n",
                "        audio_data = np.frombuffer(frame.data, dtype=np.int16).astype(np.float32) / 32768.0\n",
                "        \n",
                "        # Transcribe\n",
                "        try:\n",
                "            segments, _ = WHISPER.transcribe(audio_data, beam_size=1, language=\"en\")\n",
                "            text = \" \".join(s.text for s in segments).strip()\n",
                "            print(f\"ðŸŽ¤ User: {text}\")\n",
                "            \n",
                "            return stt.SpeechEvent(\n",
                "                type=stt.SpeechEventType.FINAL_TRANSCRIPT,\n",
                "                alternatives=[stt.SpeechData(text=text, language=\"en\")],\n",
                "            )\n",
                "        except Exception as e:\n",
                "            print(f\"STT error: {e}\")\n",
                "            return stt.SpeechEvent(\n",
                "                type=stt.SpeechEventType.FINAL_TRANSCRIPT,\n",
                "                alternatives=[stt.SpeechData(text=\"\", language=\"en\")],\n",
                "            )\n",
                "\n",
                "\n",
                "class VoiceAgent(Agent):\n",
                "    def __init__(self):\n",
                "        super().__init__(instructions=\"You are a helpful voice assistant. Keep responses short, 1-2 sentences.\")\n",
                "        self._sent_re = re.compile(r\"(.+?[.!?]\\s+|.+?\\n+)\", re.DOTALL)\n",
                "\n",
                "    async def stt_node(self, audio: AsyncIterable[rtc.AudioFrame], model_settings: ModelSettings):\n",
                "        \"\"\"Custom STT using local Whisper\"\"\"\n",
                "        chunks = []\n",
                "        async for frame in audio:\n",
                "            audio_data = np.frombuffer(frame.data, dtype=np.int16)\n",
                "            chunks.append(audio_data)\n",
                "        \n",
                "        if not chunks:\n",
                "            yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "            return\n",
                "        \n",
                "        # Combine and normalize\n",
                "        audio_np = np.concatenate(chunks).astype(np.float32) / 32768.0\n",
                "        \n",
                "        # Transcribe with Whisper\n",
                "        try:\n",
                "            segments, _ = WHISPER.transcribe(audio_np, beam_size=1, language=\"en\")\n",
                "            text = \" \".join(s.text for s in segments).strip()\n",
                "            print(f\"ðŸŽ¤ User: {text}\")\n",
                "            \n",
                "            if text:\n",
                "                yield stt.SpeechEvent(\n",
                "                    type=stt.SpeechEventType.FINAL_TRANSCRIPT,\n",
                "                    alternatives=[stt.SpeechData(text=text, language=\"en\")],\n",
                "                )\n",
                "        except Exception as e:\n",
                "            print(f\"STT error: {e}\")\n",
                "        \n",
                "        yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "\n",
                "    async def tts_node(self, text: AsyncIterable[str], model_settings: ModelSettings):\n",
                "        \"\"\"Custom TTS using Soprano\"\"\"\n",
                "        buffer = \"\"\n",
                "        sr, spf = 32000, 640\n",
                "\n",
                "        def to_frames(pcm: np.ndarray):\n",
                "            pcm = np.clip(pcm, -1.0, 1.0)\n",
                "            pcm_i16 = (pcm * 32767).astype(np.int16)\n",
                "            for i in range(0, len(pcm_i16), spf):\n",
                "                chunk = pcm_i16[i:i+spf]\n",
                "                if len(chunk) < spf:\n",
                "                    chunk = np.pad(chunk, (0, spf - len(chunk)))\n",
                "                yield rtc.AudioFrame(\n",
                "                    data=chunk.tobytes(),\n",
                "                    sample_rate=sr,\n",
                "                    num_channels=1,\n",
                "                    samples_per_channel=spf\n",
                "                )\n",
                "\n",
                "        async def speak(sentence: str):\n",
                "            sentence = sentence.strip()\n",
                "            if not sentence:\n",
                "                return\n",
                "            print(f\"ðŸ”Š Speaking: {sentence}\")\n",
                "            try:\n",
                "                for chunk in SOPRANO.infer_stream(sentence, chunk_size=1):\n",
                "                    pcm = np.asarray(chunk, dtype=np.float32)\n",
                "                    for frame in to_frames(pcm):\n",
                "                        yield frame\n",
                "            except Exception as e:\n",
                "                print(f\"TTS error: {e}\")\n",
                "\n",
                "        async for delta in text:\n",
                "            buffer += delta\n",
                "            while (m := self._sent_re.match(buffer)):\n",
                "                sentence = m.group(1)\n",
                "                buffer = buffer[len(sentence):]\n",
                "                async for frame in speak(sentence):\n",
                "                    yield frame\n",
                "        if buffer.strip():\n",
                "            async for frame in speak(buffer):\n",
                "                yield frame\n",
                "\n",
                "\n",
                "async def entrypoint(ctx: agents.JobContext):\n",
                "    # Load models\n",
                "    load_models()\n",
                "    \n",
                "    # Connect\n",
                "    await ctx.connect()\n",
                "    print(f\"âœ… Connected to room: {ctx.room.name}\")\n",
                "    \n",
                "    # VAD\n",
                "    print(\"Loading Silero VAD...\")\n",
                "    vad = silero.VAD.load(min_speech_duration=0.05, min_silence_duration=0.4)\n",
                "    print(\"âœ… VAD ready\")\n",
                "    \n",
                "    agent = VoiceAgent()\n",
                "    \n",
                "    # Use google.LLM (works with just API key!)\n",
                "    # Do NOT pass stt= here, let the agent's stt_node handle it\n",
                "    session = AgentSession(\n",
                "        turn_detection=\"vad\",\n",
                "        vad=vad,\n",
                "        llm=google.LLM(model=\"gemini-2.0-flash\"),\n",
                "        # STT and TTS are handled by stt_node/tts_node in VoiceAgent\n",
                "    )\n",
                "    \n",
                "    await session.start(agent=agent, room=ctx.room)\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"ðŸŽ¤ LISTENING... Speak now!\")\n",
                "    print(\"=\"*50 + \"\\n\")\n",
                "    \n",
                "    # Keep alive\n",
                "    disconnect_event = asyncio.Event()\n",
                "    \n",
                "    @ctx.room.on(\"disconnected\")\n",
                "    def on_disconnect():\n",
                "        print(\"Room disconnected\")\n",
                "        disconnect_event.set()\n",
                "    \n",
                "    await disconnect_event.wait()\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    cli.run_app(\n",
                "        agents.WorkerOptions(\n",
                "            entrypoint_fnc=entrypoint,\n",
                "        )\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python agent_v12.py start"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ”§ What's Different\n",
                "\n",
                "- **LLM**: Uses `google.LLM()` which ONLY needs `GOOGLE_API_KEY` (no Cloud auth!)\n",
                "- **STT**: Custom `stt_node` using local Faster Whisper\n",
                "- **TTS**: Custom `tts_node` using Soprano\n",
                "\n",
                "The key is that `google.LLM()` uses the Gemini REST API (API key only), while `google.STT()` uses Cloud Speech-to-Text (needs service account). We only use the LLM!\n",
                "\n",
                "```\n",
                "ðŸŽ™ï¸ Your Voice\n",
                "    â†“ Faster Whisper (local)\n",
                "ðŸ§  Gemini 2.0 Flash (API key)\n",
                "    â†“ Soprano TTS (local)\n",
                "ðŸ”Š Agent Speaks\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}