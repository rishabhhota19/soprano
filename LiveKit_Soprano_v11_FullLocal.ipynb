{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¤ LiveKit Voice Agent v11 - Fully Local STT\n",
                "\n",
                "**Whisper (local) â†’ Gemini â†’ Soprano TTS**\n",
                "\n",
                "âš ï¸ This version avoids livekit.plugins.google to prevent Google Cloud STT auth errors!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q \"livekit-agents[silero]~=1.3\" soprano-tts faster-whisper google-generativeai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# LiveKit credentials\n",
                "os.environ[\"LIVEKIT_URL\"] = \"wss://test-jllkasbg.livekit.cloud\"\n",
                "os.environ[\"LIVEKIT_API_KEY\"] = \"APIFnsAaWh3eFdR\"\n",
                "os.environ[\"LIVEKIT_API_SECRET\"] = \"WabCvkbupgaGfV7JQKBdZNDlYXuRFrr9jZcu7HTFdfG\"\n",
                "\n",
                "# Gemini API Key\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD9sGx9FmvzIl7NtgU7vdwJVgs7NohSSqI\"\n",
                "\n",
                "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
                "\n",
                "print(\"âœ… Credentials set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile agent_v11.py\n",
                "\"\"\"LiveKit Voice Agent v11: Local Whisper + Gemini API + Soprano TTS\"\"\"\n",
                "\n",
                "import asyncio\n",
                "import os\n",
                "import re\n",
                "import numpy as np\n",
                "from typing import AsyncIterable\n",
                "\n",
                "from livekit import agents, rtc\n",
                "from livekit.agents import Agent, AgentSession, ModelSettings, cli, llm\n",
                "from livekit.plugins import silero\n",
                "\n",
                "import google.generativeai as genai\n",
                "\n",
                "# Global models\n",
                "SOPRANO = None\n",
                "WHISPER = None\n",
                "GEMINI = None\n",
                "\n",
                "\n",
                "def load_models():\n",
                "    global SOPRANO, WHISPER, GEMINI\n",
                "    \n",
                "    if WHISPER is None:\n",
                "        print(\"Loading Faster Whisper...\")\n",
                "        from faster_whisper import WhisperModel\n",
                "        WHISPER = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
                "        print(\"âœ… Whisper ready\")\n",
                "    \n",
                "    if SOPRANO is None:\n",
                "        print(\"Loading Soprano TTS...\")\n",
                "        from soprano import SopranoTTS\n",
                "        SOPRANO = SopranoTTS(device=\"cuda\")\n",
                "        print(\"âœ… Soprano TTS ready\")\n",
                "    \n",
                "    if GEMINI is None:\n",
                "        print(\"Configuring Gemini...\")\n",
                "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
                "        GEMINI = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
                "        print(\"âœ… Gemini ready\")\n",
                "\n",
                "\n",
                "class SimpleLLM(llm.LLM):\n",
                "    \"\"\"Simple LLM wrapper for Gemini using google-generativeai directly\"\"\"\n",
                "    \n",
                "    def chat(self, chat_ctx, fnc_ctx=None, **kwargs):\n",
                "        async def _generate():\n",
                "            # Build messages\n",
                "            messages = []\n",
                "            for msg in chat_ctx.messages:\n",
                "                role = \"user\" if msg.role == \"user\" else \"model\"\n",
                "                if isinstance(msg.content, str):\n",
                "                    messages.append({\"role\": role, \"parts\": [msg.content]})\n",
                "            \n",
                "            # Stream response\n",
                "            chat = GEMINI.start_chat(history=messages[:-1] if len(messages) > 1 else [])\n",
                "            last_msg = messages[-1][\"parts\"][0] if messages else \"Hello\"\n",
                "            \n",
                "            response = chat.send_message(last_msg, stream=True)\n",
                "            \n",
                "            for chunk in response:\n",
                "                if chunk.text:\n",
                "                    yield llm.ChatChunk(\n",
                "                        choices=[llm.Choice(delta=llm.ChoiceDelta(content=chunk.text))]\n",
                "                    )\n",
                "        \n",
                "        return llm.LLMStream(_generate(), chat_ctx=chat_ctx, fnc_ctx=fnc_ctx)\n",
                "\n",
                "\n",
                "class VoiceAgent(Agent):\n",
                "    def __init__(self):\n",
                "        super().__init__(instructions=\"You are a helpful voice assistant. Keep responses short, 1-2 sentences.\")\n",
                "        self._sent_re = re.compile(r\"(.+?[.!?]\\s+|.+?\\n+)\", re.DOTALL)\n",
                "\n",
                "    async def stt_node(self, audio: AsyncIterable[rtc.AudioFrame], model_settings: ModelSettings):\n",
                "        \"\"\"Custom STT using local Whisper\"\"\"\n",
                "        from livekit.agents import stt\n",
                "        \n",
                "        chunks = []\n",
                "        async for frame in audio:\n",
                "            # Convert frame data to numpy\n",
                "            audio_data = np.frombuffer(frame.data, dtype=np.int16)\n",
                "            chunks.append(audio_data)\n",
                "        \n",
                "        if not chunks:\n",
                "            yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "            return\n",
                "        \n",
                "        # Combine and normalize\n",
                "        audio_np = np.concatenate(chunks).astype(np.float32) / 32768.0\n",
                "        \n",
                "        # Transcribe with Whisper\n",
                "        try:\n",
                "            segments, _ = WHISPER.transcribe(audio_np, beam_size=1, language=\"en\")\n",
                "            text = \" \".join(s.text for s in segments).strip()\n",
                "            print(f\"ðŸŽ¤ User: {text}\")\n",
                "            \n",
                "            if text:\n",
                "                yield stt.SpeechEvent(\n",
                "                    type=stt.SpeechEventType.FINAL_TRANSCRIPT,\n",
                "                    alternatives=[stt.SpeechData(text=text, language=\"en\")],\n",
                "                )\n",
                "        except Exception as e:\n",
                "            print(f\"STT error: {e}\")\n",
                "        \n",
                "        yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "\n",
                "    async def tts_node(self, text: AsyncIterable[str], model_settings: ModelSettings):\n",
                "        \"\"\"Custom TTS using Soprano\"\"\"\n",
                "        buffer = \"\"\n",
                "        sr, spf = 32000, 640\n",
                "\n",
                "        def to_frames(pcm: np.ndarray):\n",
                "            pcm = np.clip(pcm, -1.0, 1.0)\n",
                "            pcm_i16 = (pcm * 32767).astype(np.int16)\n",
                "            for i in range(0, len(pcm_i16), spf):\n",
                "                chunk = pcm_i16[i:i+spf]\n",
                "                if len(chunk) < spf:\n",
                "                    chunk = np.pad(chunk, (0, spf - len(chunk)))\n",
                "                yield rtc.AudioFrame(\n",
                "                    data=chunk.tobytes(),\n",
                "                    sample_rate=sr,\n",
                "                    num_channels=1,\n",
                "                    samples_per_channel=spf\n",
                "                )\n",
                "\n",
                "        async def speak(sentence: str):\n",
                "            sentence = sentence.strip()\n",
                "            if not sentence:\n",
                "                return\n",
                "            print(f\"ðŸ”Š Speaking: {sentence}\")\n",
                "            try:\n",
                "                for chunk in SOPRANO.infer_stream(sentence, chunk_size=1):\n",
                "                    pcm = np.asarray(chunk, dtype=np.float32)\n",
                "                    for frame in to_frames(pcm):\n",
                "                        yield frame\n",
                "            except Exception as e:\n",
                "                print(f\"TTS error: {e}\")\n",
                "\n",
                "        async for delta in text:\n",
                "            buffer += delta\n",
                "            while (m := self._sent_re.match(buffer)):\n",
                "                sentence = m.group(1)\n",
                "                buffer = buffer[len(sentence):]\n",
                "                async for frame in speak(sentence):\n",
                "                    yield frame\n",
                "        if buffer.strip():\n",
                "            async for frame in speak(buffer):\n",
                "                yield frame\n",
                "\n",
                "\n",
                "async def entrypoint(ctx: agents.JobContext):\n",
                "    # Load all models\n",
                "    load_models()\n",
                "    \n",
                "    # Connect to room\n",
                "    await ctx.connect()\n",
                "    print(f\"âœ… Connected to room: {ctx.room.name}\")\n",
                "    \n",
                "    # Load VAD\n",
                "    print(\"Loading Silero VAD...\")\n",
                "    vad = silero.VAD.load(min_speech_duration=0.05, min_silence_duration=0.4)\n",
                "    print(\"âœ… VAD ready\")\n",
                "    \n",
                "    agent = VoiceAgent()\n",
                "    \n",
                "    session = AgentSession(\n",
                "        turn_detection=\"vad\",\n",
                "        vad=vad,\n",
                "        llm=SimpleLLM(),  # Custom Gemini wrapper\n",
                "        # STT and TTS handled by stt_node/tts_node in VoiceAgent\n",
                "    )\n",
                "    \n",
                "    await session.start(agent=agent, room=ctx.room)\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"ðŸŽ¤ LISTENING... Speak now!\")\n",
                "    print(\"=\"*50 + \"\\n\")\n",
                "    \n",
                "    # Keep alive\n",
                "    disconnect_event = asyncio.Event()\n",
                "    \n",
                "    @ctx.room.on(\"disconnected\")\n",
                "    def on_disconnect():\n",
                "        print(\"Room disconnected\")\n",
                "        disconnect_event.set()\n",
                "    \n",
                "    await disconnect_event.wait()\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    cli.run_app(\n",
                "        agents.WorkerOptions(\n",
                "            entrypoint_fnc=entrypoint,\n",
                "        )\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python agent_v11.py start"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ”§ What's Different in v11\n",
                "\n",
                "| Component | v10 | v11 |\n",
                "|-----------|-----|-----|\n",
                "| STT | Whisper API (separate process) | Whisper (in-process) |\n",
                "| LLM | `livekit.plugins.google` | `google-generativeai` directly |\n",
                "| TTS | Soprano | Soprano |\n",
                "\n",
                "**Key fix**: No `livekit.plugins.google` import = no Google Cloud auth errors!\n",
                "\n",
                "## Pipeline\n",
                "```\n",
                "ðŸŽ™ï¸ Your Voice\n",
                "    â†“\n",
                "Faster Whisper (local GPU)\n",
                "    â†“\n",
                "Gemini 2.0 Flash (via API key)\n",
                "    â†“\n",
                "Soprano TTS (local GPU)\n",
                "    â†“\n",
                "ðŸ”Š Agent Speaks\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}