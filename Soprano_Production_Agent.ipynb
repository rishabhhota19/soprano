{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéôÔ∏è Soprano Production-Grade Full-Duplex Agent\n",
                "\n",
                "This notebook implements a high-performance **Speech-to-Speech** agent using:\n",
                "- **Server**: FastAPI + WebSockets (Full Duplex)\n",
                "- **STT**: Faster-Whisper (GPU optimized)\n",
                "- **TTS**: Soprano (Streaming, Low Latency)\n",
                "- **VAD**: Silero VAD (Fast CPU)\n",
                "- **Transport**: Binary PCM over WebSockets\n",
                "- **Client**: WebRTC-style microphone capture with **Echo Cancellation** (AEC)\n",
                "\n",
                "### Architecture\n",
                "1. **Browser** captures audio -> sends PCM to Server.\n",
                "2. **Server** detects speech (VAD) -> **Interruption Spike** (stops TTS) -> STT -> LLM -> TTS Stream.\n",
                "3. **Browser** receives PCM -> Queues for playback -> Flushes queue on interruption signal.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1. üì¶ Setup & Installation\n",
                "#@markdown **Run this cell first, then RESTART RUNTIME.**\n",
                "\n",
                "print(\"üîß Installing dependencies...\")\n",
                "\n",
                "# 0. LOCK NUMPY VERSION (Disabled)\n",
                "# print(\"üìå Locking NumPy to 1.26.4...\")\n",
                "# !pip uninstall numpy -y --quiet 2>/dev/null\n",
                "# !pip install \"numpy==1.26.4\" --quiet\n",
                "\n",
                "# 1. Clone Soprano\n",
                "!git clone https://github.com/ekwek1/soprano.git 2>/dev/null || echo \"Soprano already cloned\"\n",
                "%cd soprano\n",
                "\n",
                "# 2. Install Soprano & Core Deps\n",
                "!pip install -e . --quiet\n",
                "!pip install transformers huggingface_hub scipy unidecode --quiet\n",
                "\n",
                "# 3. Install Server & Agent Deps\n",
                "!pip install fastapi uvicorn[standard] websockets soundfile --quiet\n",
                "!pip install faster-whisper --quiet\n",
                "\n",
                "# 4. Install Cloudflared for Tunneling\n",
                "!pip install pycloudflared --quiet\n",
                "\n",
                "# 5. Force Reinstall NumPy safely at the end (Disabled)\n",
                "# !pip install \"numpy==1.26.4\" --force-reinstall --quiet\n",
                "\n",
                "print(\"‚úÖ Installation complete!\")\n",
                "print(\"‚ö†Ô∏è  IMPORTANT: Go to 'Runtime -> Restart runtime' now!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2. üìù Create Server Code (app.py)\n",
                "#@markdown Writes the FastAPI server code to `app.py`.\n",
                "\n",
                "code = \"\"\"\n",
                "import asyncio\n",
                "import json\n",
                "import time\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, Deque\n",
                "from collections import deque\n",
                "import os\n",
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
                "from fastapi.responses import HTMLResponse\n",
                "from faster_whisper import WhisperModel\n",
                "\n",
                "# ---- Soprano ----\n",
                "try:\n",
                "    from soprano import SopranoTTS\n",
                "except ImportError:\n",
                "    # Fallback if running from root dir\n",
                "    sys.path.append(os.getcwd())\n",
                "    from soprano import SopranoTTS\n",
                "\n",
                "# ---- Audio config ----\n",
                "IN_SR = 16000            # mic sample rate\n",
                "OUT_SR = 32000           # soprano output rate\n",
                "\n",
                "# ---- VAD (Silero) ----\n",
                "torch.set_num_threads(1)\n",
                "vad_model, vad_utils = torch.hub.load(repo_or_dir=\\\"snakers4/silero-vad\\\", model=\\\"silero_vad\\\", trust_repo=True)\n",
                "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = vad_utils\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "@dataclass\n",
                "class SessionState:\n",
                "    # mic audio\n",
                "    speech_buf: Deque[np.ndarray]\n",
                "    vad: VADIterator\n",
                "    in_speech: bool\n",
                "    # tts control\n",
                "    tts_task: Optional[asyncio.Task]\n",
                "    tts_cancel: asyncio.Event\n",
                "\n",
                "def pcm16_to_float32(pcm: bytes) -> np.ndarray:\n",
                "    x = np.frombuffer(pcm, dtype=np.int16).astype(np.float32)\n",
                "    return (x / 32768.0).clip(-1.0, 1.0)\n",
                "\n",
                "def float32_to_pcm16(x: np.ndarray) -> bytes:\n",
                "    x = np.clip(x, -1.0, 1.0)\n",
                "    i16 = (x * 32767.0).astype(np.int16)\n",
                "    return i16.tobytes()\n",
                "\n",
                "def generate_response_text(user_text: str) -> str:\n",
                "    # TODO: Hook up Gemini here via google-generativeai\n",
                "    # For low-latency demo, we use a mock or simple logic\n",
                "    return f\\\"I heard you say: {user_text}. This is a full duplex test.\\\"\n",
                "\n",
                "async def stream_soprano_tts(ws: WebSocket, tts: SopranoTTS, text: str, st: SessionState):\n",
                "    \\\"\\\"\\\"\n",
                "    Stream TTS chunks to client. Supports cancellation for barge-in.\n",
                "    \\\"\\\"\\\"\n",
                "    await ws.send_text(json.dumps({\\\"type\\\": \\\"tts_start\\\", \\\"sample_rate\\\": OUT_SR}))\n",
                "\n",
                "    # Soprano streaming\n",
                "    for chunk in tts.infer_stream(text, chunk_size=1, temperature=0.0):\n",
                "        if st.tts_cancel.is_set():\n",
                "            break\n",
                "\n",
                "        # chunk may be torch tensor\n",
                "        if hasattr(chunk, \\\"detach\\\"):\n",
                "            chunk = chunk.detach().cpu().numpy()\n",
                "\n",
                "        # Ensure 1D float32\n",
                "        chunk = np.asarray(chunk, dtype=np.float32).reshape(-1)\n",
                "        await ws.send_bytes(float32_to_pcm16(chunk))\n",
                "        await asyncio.sleep(0) # Yield\n",
                "\n",
                "    await ws.send_text(json.dumps({\\\"type\\\": \\\"tts_end\\\"}))\n",
                "\n",
                "async def cancel_tts(ws: WebSocket, st: SessionState):\n",
                "    \\\"\\\"\\\"\n",
                "    Server-side cancel + client-side float flush.\n",
                "    \\\"\\\"\\\"\n",
                "    st.tts_cancel.set()\n",
                "    if st.tts_task and not st.tts_task.done():\n",
                "        st.tts_task.cancel()\n",
                "        try:\n",
                "            await st.tts_task\n",
                "        except:\n",
                "            pass\n",
                "    st.tts_task = None\n",
                "    await ws.send_text(json.dumps({\\\"type\\\": \\\"stop_audio\\\"}))\n",
                "\n",
                "def make_whisper_model():\n",
                "    return WhisperModel(\\\"base\\\", device=\\\"cuda\\\", compute_type=\\\"float16\\\")\n",
                "\n",
                "@app.get(\\\"/\\\")\n",
                "async def get():\n",
                "    # returns the client HTML\n",
                "    with open(\\\"index.html\\\", \\\"r\\\") as f:\n",
                "        return HTMLResponse(f.read())\n",
                "\n",
                "@app.websocket(\\\"/ws\\\")\n",
                "async def ws_endpoint(ws: WebSocket):\n",
                "    await ws.accept()\n",
                "\n",
                "    # Initialize models (in prod, load these globally/startup event)\n",
                "    # For Colab demo, loading here ensures GPU is visible\n",
                "    if not hasattr(app.state, 'tts'):\n",
                "        print(\\\"Loading Soprano...\\\")\n",
                "        app.state.tts = SopranoTTS(device=\\\"cuda\\\", backend=\\\"auto\\\")\n",
                "        print(\\\"Loading Whisper...\\\")\n",
                "        app.state.whisper = make_whisper_model()\n",
                "\n",
                "    tts = app.state.tts\n",
                "    whisper = app.state.whisper\n",
                "    vad_it = VADIterator(vad_model, sampling_rate=IN_SR)\n",
                "\n",
                "    st = SessionState(\n",
                "        speech_buf=deque(),\n",
                "        vad=vad_it,\n",
                "        in_speech=False,\n",
                "        tts_task=None,\n",
                "        tts_cancel=asyncio.Event(),\n",
                "    )\n",
                "\n",
                "    # Hello message\n",
                "    hello = \\\"I'm online. You can speak to me, and interrupt me, at any time.\\\"\n",
                "    st.tts_cancel.clear()\n",
                "    st.tts_task = asyncio.create_task(stream_soprano_tts(ws, tts, hello, st))\n",
                "\n",
                "    try:\n",
                "        while True:\n",
                "            msg = await ws.receive()\n",
                "\n",
                "            if \\\"bytes\\\" in msg and msg[\\\"bytes\\\"]:\n",
                "                pcm = msg[\\\"bytes\\\"]\n",
                "                x = pcm16_to_float32(pcm)\n",
                "                tx = torch.from_numpy(x)\n",
                "\n",
                "                speech_event = st.vad(tx)\n",
                "\n",
                "                if speech_event:\n",
                "                    if speech_event.get(\\\"start\\\"):\n",
                "                        # User started talking -> STOP TTS\n",
                "                        if not st.in_speech:\n",
                "                            st.in_speech = True\n",
                "                            print(\\\"[VAD] Speech Start -> Interrupting\\\")\n",
                "                            await cancel_tts(ws, st)\n",
                "                    \n",
                "                    if speech_event.get(\\\"end\\\"):\n",
                "                        if st.in_speech:\n",
                "                            st.in_speech = False\n",
                "                            print(\\\"[VAD] Speech End -> Transcribing\\\")\n",
                "                            \n",
                "                            if len(st.speech_buf) > 0:\n",
                "                                utter = np.concatenate(list(st.speech_buf))\n",
                "                                st.speech_buf.clear()\n",
                "\n",
                "                                # STT\n",
                "                                segments, _ = whisper.transcribe(utter, beam_size=1, vad_filter=False)\n",
                "                                user_text = \\\" \\\".join([s.text.strip() for s in segments]).strip()\n",
                "                                \n",
                "                                if user_text:\n",
                "                                    await ws.send_text(json.dumps({\\\"type\\\": \\\"user_text\\\", \\\"text\\\": user_text}))\n",
                "                                    \n",
                "                                    # Response\n",
                "                                    resp = generate_response_text(user_text)\n",
                "                                    await ws.send_text(json.dumps({\\\"type\\\": \\\"agent_text\\\", \\\"text\\\": resp}))\n",
                "\n",
                "                                    # Stream TTS\n",
                "                                    st.tts_cancel.clear()\n",
                "                                    st.tts_task = asyncio.create_task(stream_soprano_tts(ws, tts, resp, st))\n",
                "\n",
                "                if st.in_speech:\n",
                "                    st.speech_buf.append(x)\n",
                "    except WebSocketDisconnect:\n",
                "        print(\\\"Client disconnected\\\")\n",
                "    finally:\n",
                "        await cancel_tts(ws, st)\n",
                "\"\"\"\n",
                "\n",
                "with open(\"app.py\", \"w\") as f:\n",
                "    f.write(code)\n",
                "\n",
                "print(\"‚úÖ app.py created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3. üìù Create Client Code (index.html)\n",
                "#@markdown Writes the WebRTC client code.\n",
                "\n",
                "html_code = \"\"\"\n",
                "<!doctype html>\n",
                "<html>\n",
                "<head>\n",
                "  <meta charset=\\\"utf-8\\\"/>\n",
                "  <title>Soprano Full Duplex</title>\n",
                "  <style>\n",
                "    body { font-family: system-ui; max-width: 800px; margin: 2rem auto; padding: 0 1rem; background: #f4f4f4; }\n",
                "    .container { background: white; padding: 2rem; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n",
                "    button { font-size: 1.2rem; padding: 10px 20px; cursor: pointer; background: #007bff; color: white; border: none; border-radius: 5px; }\n",
                "    button:disabled { background: #ccc; }\n",
                "    #status { margin-top: 1rem; font-weight: bold; color: #555; }\n",
                "    #log { background: #1e1e1e; color: #0f0; padding: 1rem; border-radius: 5px; height: 300px; overflow-y: auto; font-family: monospace; margin-top: 1rem; }\n",
                "  </style>\n",
                "</head>\n",
                "<body>\n",
                "<div class=\\\"container\\\">\n",
                "  <h2>üéôÔ∏è Soprano Full Duplex Agent</h2>\n",
                "  <p>Speak naturally. Interrupt anytime.</p>\n",
                "  <button id=\\\"go\\\">Start Conversation</button>\n",
                "  <div id=\\\"status\\\">Disconnected</div>\n",
                "  <div id=\\\"log\\\"></div>\n",
                "</div>\n",
                "<script>\n",
                "// Detect environment to set WS URL automatically\n",
                "const proto = window.location.protocol === 'https:' ? 'wss' : 'ws';\n",
                "const WS_URL = `${proto}://${window.location.host}/ws`;\n",
                "\n",
                "const logEl = document.getElementById(\\\"log\\\");\n",
                "const log = (s) => {\n",
                "  logEl.textContent += `> ${s}\\n`;\n",
                "  logEl.scrollTop = logEl.scrollHeight;\n",
                "};\n",
                "\n",
                "let audioCtx, source, processor;\n",
                "let ttsQueue = [];\n",
                "let playing = false;\n",
                "let outSR = 32000;\n",
                "\n",
                "function clearPlayback() {\n",
                "  ttsQueue = [];\n",
                "  playing = false;\n",
                "  // Note: Web Audio API buffers can't be strictly \\\"cancelled\\\" once scheduled easily without replacing nodes,\n",
                "  // but clearing the queue prevents future chunks. For strictly immediate stop, we'd close/reopen context or disconnect node.\n",
                "  // For this demo, clearing queue + state is sufficient for perceived interruption.\n",
                "}\n",
                "\n",
                "async function pumpPlayback() {\n",
                "  if (playing) return;\n",
                "  playing = true;\n",
                "\n",
                "  while (ttsQueue.length > 0) {\n",
                "    if (!playing) break; // Check interrupt\n",
                "    \n",
                "    const pcm16 = ttsQueue.shift();\n",
                "    const i16 = new Int16Array(pcm16);\n",
                "    const f32 = new Float32Array(i16.length);\n",
                "    for (let i=0; i<i16.length; i++) f32[i] = i16[i] / 32768.0;\n",
                "\n",
                "    const buf = audioCtx.createBuffer(1, f32.length, outSR);\n",
                "    buf.getChannelData(0).set(f32);\n",
                "\n",
                "    const src = audioCtx.createBufferSource();\n",
                "    src.buffer = buf;\n",
                "    src.connect(audioCtx.destination);\n",
                "    src.start();\n",
                "\n",
                "    await new Promise(res => src.onended = res);\n",
                "  }\n",
                "  playing = false;\n",
                "}\n",
                "\n",
                "document.getElementById(\\\"go\\\").onclick = async () => {\n",
                "  // 1. Audio Context\n",
                "  audioCtx = new (window.AudioContext || window.webkitAudioContext)();\n",
                "  \n",
                "  // 2. WebSocket\n",
                "  document.getElementById(\\\"status\\\").textContent = \\\"Connecting...\\\";\n",
                "  const ws = new WebSocket(WS_URL);\n",
                "  ws.binaryType = \\\"arraybuffer\\\";\n",
                "\n",
                "  ws.onopen = async () => {\n",
                "    document.getElementById(\\\"status\\\").textContent = \\\"Connected - Listening\\\";\n",
                "    document.getElementById(\\\"go\\\").disabled = true;\n",
                "    log(\\\"Ws Connected. Requesting Mic...\\\");\n",
                "\n",
                "    // 3. Mic Capture with AEC (Critical for full duplex)\n",
                "    const stream = await navigator.mediaDevices.getUserMedia({\n",
                "      audio: {\n",
                "        channelCount: 1,\n",
                "        sampleRate: 16000,\n",
                "        echoCancellation: true, \n",
                "        noiseSuppression: true,\n",
                "        autoGainControl: true\n",
                "      }\n",
                "    });\n",
                "\n",
                "    // 4. Processor (Worklet is better, but ScriptProcessor is easier for single-file demo)\n",
                "    source = audioCtx.createMediaStreamSource(stream);\n",
                "    processor = audioCtx.createScriptProcessor(2048, 1, 1);\n",
                "\n",
                "    processor.onaudioprocess = (e) => {\n",
                "      if (ws.readyState !== WebSocket.OPEN) return;\n",
                "      \n",
                "      const input = e.inputBuffer.getChannelData(0);\n",
                "      // Float32 -> Int16\n",
                "      const pcm = new Int16Array(input.length);\n",
                "      for (let i=0; i<input.length; i++) {\n",
                "        let s = Math.max(-1, Math.min(1, input[i]));\n",
                "        pcm[i] = s < 0 ? s * 32768 : s * 32767;\n",
                "      }\n",
                "      ws.send(pcm.buffer);\n",
                "    };\n",
                "\n",
                "    source.connect(processor);\n",
                "    processor.connect(audioCtx.destination);\n",
                "    log(\\\"Mic active. Speak anytime!\\\");\n",
                "  };\n",
                "\n",
                "  ws.onmessage = (ev) => {\n",
                "    if (typeof ev.data === \\\"string\\\") {\n",
                "      const msg = JSON.parse(ev.data);\n",
                "      \n",
                "      if (msg.type === \\\"tts_start\\\") outSR = msg.sample_rate;\n",
                "      if (msg.type === \\\"stop_audio\\\") {\n",
                "        log(\\\"üõë INTERRUPT Detected -> Clearing playback\\\");\n",
                "        clearPlayback();\n",
                "      }\n",
                "      if (msg.type === \\\"user_text\\\") log(\\\"User: \\\" + msg.text);\n",
                "      if (msg.type === \\\"agent_text\\\") log(\\\"Agent: \\\" + msg.text);\n",
                "      return;\n",
                "    }\n",
                "\n",
                "    // Binary Audio\n",
                "    ttsQueue.push(ev.data);\n",
                "    pumpPlayback();\n",
                "  };\n",
                "\n",
                "  ws.onclose = () => {\n",
                "    document.getElementById(\\\"status\\\").textContent = \\\"Disconnected\\\";\n",
                "    log(\\\"Disconnected\\\");\n",
                "  };\n",
                "};\n",
                "</script>\n",
                "</body>\n",
                "</html>\n",
                "\"\"\"\n",
                "\n",
                "with open(\"index.html\", \"w\") as f:\n",
                "    f.write(html_code)\n",
                "\n",
                "print(\"‚úÖ index.html created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4. üöÄ Launch Server\n",
                "#@markdown Starts the FastAPI server and exposes it via Cloudflare Tunnel.\n",
                "#@markdown Click the **trycloudflare** link that appears to access the UI.\n",
                "\n",
                "import os\n",
                "import threading\n",
                "import time\n",
                "from pycloudflared import try_cloudflare\n",
                "\n",
                "# 1. Start Tunnel\n",
                "public_url = try_cloudflare(port=8000)\n",
                "print(f\"\\nüåç Public URL: {public_url}\\n\")\n",
                "\n",
                "# 2. Run Uvicorn (Blocking)\n",
                "print(\"üöÄ Starting Uvicorn Server... (This buffer will capture logs)\")\n",
                "!uvicorn app:app --host 0.0.0.0 --port 8000"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
