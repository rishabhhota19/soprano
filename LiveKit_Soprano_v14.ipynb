{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¤ LiveKit Voice Agent v14 - FIXED\n",
                "\n",
                "**Whisper + Gemini + Soprano**\n",
                "\n",
                "Fixes: Empty prompt error with greeting + turn detection tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q \"livekit-agents[google,silero]~=1.3\" soprano-tts faster-whisper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"LIVEKIT_URL\"] = \"wss://test-jllkasbg.livekit.cloud\"\n",
                "os.environ[\"LIVEKIT_API_KEY\"] = \"APIFnsAaWh3eFdR\"\n",
                "os.environ[\"LIVEKIT_API_SECRET\"] = \"WabCvkbupgaGfV7JQKBdZNDlYXuRFrr9jZcu7HTFdfG\"\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD9sGx9FmvzIl7NtgU7vdwJVgs7NohSSqI\"\n",
                "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
                "print(\"âœ… Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile agent_v14.py\n",
                "import asyncio\n",
                "import re\n",
                "import numpy as np\n",
                "from typing import AsyncIterable\n",
                "\n",
                "from livekit import agents, rtc\n",
                "from livekit.agents import Agent, AgentSession, ModelSettings, cli, stt\n",
                "from livekit.plugins import google, silero\n",
                "\n",
                "SOPRANO = None\n",
                "WHISPER = None\n",
                "\n",
                "def load_models():\n",
                "    global SOPRANO, WHISPER\n",
                "    if WHISPER is None:\n",
                "        print(\"Loading Whisper...\")\n",
                "        from faster_whisper import WhisperModel\n",
                "        WHISPER = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
                "        print(\"âœ… Whisper ready\")\n",
                "    if SOPRANO is None:\n",
                "        print(\"Loading Soprano...\")\n",
                "        from soprano import SopranoTTS\n",
                "        SOPRANO = SopranoTTS(device=\"cuda\")\n",
                "        print(\"âœ… Soprano ready\")\n",
                "\n",
                "\n",
                "class VoiceAgent(Agent):\n",
                "    def __init__(self):\n",
                "        super().__init__(instructions=\"You are a helpful voice assistant. Keep responses short, 1-2 sentences max.\")\n",
                "        self._sent_re = re.compile(r\"(.+?[.!?]\\s+|.+?\\n+)\", re.DOTALL)\n",
                "\n",
                "    async def stt_node(self, audio: AsyncIterable[rtc.AudioFrame], model_settings: ModelSettings):\n",
                "        chunks = []\n",
                "        async for frame in audio:\n",
                "            chunks.append(np.frombuffer(frame.data, dtype=np.int16))\n",
                "        \n",
                "        if not chunks:\n",
                "            yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "            return\n",
                "        \n",
                "        audio_np = np.concatenate(chunks).astype(np.float32) / 32768.0\n",
                "        \n",
                "        # Skip very short audio (likely noise)\n",
                "        if len(audio_np) < 8000:  # < 0.5 sec at 16kHz\n",
                "            print(\"âš ï¸ Audio too short, skipping\")\n",
                "            yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "            return\n",
                "        \n",
                "        try:\n",
                "            segments, _ = WHISPER.transcribe(audio_np, beam_size=1, language=\"en\")\n",
                "            text = \" \".join(s.text for s in segments).strip()\n",
                "            \n",
                "            # Skip empty or very short transcripts\n",
                "            if not text or len(text.split()) < 1:\n",
                "                print(\"âš ï¸ Empty transcript, skipping\")\n",
                "                yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "                return\n",
                "            \n",
                "            print(f\"ðŸŽ¤ User: {text}\")\n",
                "            yield stt.SpeechEvent(\n",
                "                type=stt.SpeechEventType.FINAL_TRANSCRIPT,\n",
                "                alternatives=[stt.SpeechData(text=text, language=\"en\")],\n",
                "            )\n",
                "        except Exception as e:\n",
                "            print(f\"STT error: {e}\")\n",
                "        \n",
                "        yield stt.SpeechEvent(type=stt.SpeechEventType.END_OF_SPEECH, alternatives=[])\n",
                "\n",
                "    async def tts_node(self, text: AsyncIterable[str], model_settings: ModelSettings):\n",
                "        buffer = \"\"\n",
                "        sr, spf = 32000, 640\n",
                "\n",
                "        def to_frames(pcm):\n",
                "            pcm = np.clip(pcm, -1.0, 1.0)\n",
                "            pcm_i16 = (pcm * 32767).astype(np.int16)\n",
                "            for i in range(0, len(pcm_i16), spf):\n",
                "                chunk = pcm_i16[i:i+spf]\n",
                "                if len(chunk) < spf:\n",
                "                    chunk = np.pad(chunk, (0, spf - len(chunk)))\n",
                "                yield rtc.AudioFrame(data=chunk.tobytes(), sample_rate=sr, num_channels=1, samples_per_channel=spf)\n",
                "\n",
                "        async def speak(sentence):\n",
                "            sentence = sentence.strip()\n",
                "            if not sentence:\n",
                "                return\n",
                "            print(f\"ðŸ”Š Speaking: {sentence}\")\n",
                "            try:\n",
                "                for chunk in SOPRANO.infer_stream(sentence, chunk_size=1):\n",
                "                    for frame in to_frames(np.asarray(chunk, dtype=np.float32)):\n",
                "                        yield frame\n",
                "            except Exception as e:\n",
                "                print(f\"TTS error: {e}\")\n",
                "\n",
                "        async for delta in text:\n",
                "            buffer += delta\n",
                "            while (m := self._sent_re.match(buffer)):\n",
                "                sentence = m.group(1)\n",
                "                buffer = buffer[len(sentence):]\n",
                "                async for frame in speak(sentence):\n",
                "                    yield frame\n",
                "        if buffer.strip():\n",
                "            async for frame in speak(buffer):\n",
                "                yield frame\n",
                "\n",
                "\n",
                "async def entrypoint(ctx: agents.JobContext):\n",
                "    load_models()\n",
                "    await ctx.connect()\n",
                "    print(f\"âœ… Connected: {ctx.room.name}\")\n",
                "    \n",
                "    vad = silero.VAD.load(\n",
                "        min_speech_duration=0.1,   # Require at least 100ms of speech\n",
                "        min_silence_duration=0.5,  # Wait 500ms of silence before ending turn\n",
                "    )\n",
                "    print(\"âœ… VAD ready\")\n",
                "    \n",
                "    session = AgentSession(\n",
                "        turn_detection=\"vad\",\n",
                "        vad=vad,\n",
                "        llm=google.LLM(model=\"gemini-2.0-flash\"),\n",
                "        # Prevent empty turns from triggering LLM\n",
                "        min_interruption_words=1,\n",
                "    )\n",
                "    \n",
                "    agent = VoiceAgent()\n",
                "    await session.start(agent=agent, room=ctx.room)\n",
                "    \n",
                "    # IMPORTANT: Send greeting to seed the conversation\n",
                "    print(\"ðŸŽ¤ Sending greeting...\")\n",
                "    await session.generate_reply(\n",
                "        instructions=\"Greet the user briefly and ask how you can help today.\"\n",
                "    )\n",
                "    \n",
                "    print(\"\\nðŸŽ¤ LISTENING...\\n\")\n",
                "    \n",
                "    done = asyncio.Event()\n",
                "    ctx.room.on(\"disconnected\")(lambda: done.set())\n",
                "    await done.wait()\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    cli.run_app(agents.WorkerOptions(entrypoint_fnc=entrypoint))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python agent_v14.py start"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fixes Applied\n",
                "\n",
                "1. **Greeting on start** - Seeds conversation so Gemini has context\n",
                "2. **min_interruption_words=1** - Won't call LLM on empty transcripts\n",
                "3. **Audio length check** - Skip audio < 0.5 sec (noise)\n",
                "4. **Empty transcript check** - Skip empty Whisper results\n",
                "5. **Better VAD settings** - Require 100ms speech, wait 500ms silence"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}