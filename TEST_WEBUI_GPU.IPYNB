{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Soprano TTS - WebUI on GPU\n",
                "\n",
                "This notebook runs the Soprano TTS WebUI using the GPU acceleration provided by Google Colab.\n",
                "It installs the necessary dependencies, loads the model with the fast `lmdeploy` backend, and creates a public link to the WebUI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "301f4a05",
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1. Setup & Installation\n",
                "#@markdown This cell clones the repository and installs all necessary dependencies including `lmdeploy` for CUDA support.\n",
                "\n",
                "!git clone https://github.com/ekwek1/soprano.git\n",
                "%cd soprano\n",
                "\n",
                "# Install the package with lmdeploy support for CUDA acceleration\n",
                "print(\"Installing dependencies... This may take a minute.\")\n",
                "!pip install -e .[lmdeploy] --quiet\n",
                "\n",
                "# Ensure compatibility in Colab environment\n",
                "# We install a specific version of Gradio that is known to support audio streaming well, or just upgrade\n",
                "!pip install gradio sounddevice scipy --upgrade --quiet\n",
                "\n",
                "print(\"Installation complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56c09275",
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2. Load Model & Run WebUI\n",
                "#@markdown This cell loads the model onto the GPU and launches the Gradio WebUI. Click the public URL (e.g. `https://xxxx.gradio.live`) to access it.\n",
                "\n",
                "import time\n",
                "import gradio as gr\n",
                "import numpy as np\n",
                "import torch\n",
                "from soprano import SopranoTTS\n",
                "\n",
                "# Initialize model with GPU acceleration\n",
                "print(\"Loading Soprano TTS model (Backend: LMDeploy, Device: CUDA)...\")\n",
                "try:\n",
                "    model = SopranoTTS(\n",
                "        backend='lmdeploy',\n",
                "        device='cuda',\n",
                "        cache_size_mb=100,\n",
                "        decoder_batch_size=1\n",
                "    )\n",
                "    device = model.device\n",
                "    backend = model.backend\n",
                "    print(\"Model loaded successfully!\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Fallback: Make sure you have selected a GPU Runtime (Runtime > Change runtime type > T4 GPU).\")\n",
                "    raise e\n",
                "\n",
                "SAMPLE_RATE = 32000\n",
                "\n",
                "def generate_speech(\n",
                "    text: str,\n",
                "    temperature: float,\n",
                "    top_p: float,\n",
                "    repetition_penalty: float,\n",
                "    chunk_size: int = 1,\n",
                "    streaming: bool = False,\n",
                "):\n",
                "    if not text.strip():\n",
                "        yield None, \"Please enter some text to generate speech.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        if streaming:\n",
                "            stream = model.infer_stream(\n",
                "                text,\n",
                "                temperature=temperature,\n",
                "                top_p=top_p,\n",
                "                repetition_penalty=repetition_penalty,\n",
                "                chunk_size=chunk_size,\n",
                "            )\n",
                "            yield None, \"‚è≥ Streaming...\"\n",
                "            \n",
                "            start_time = time.time()\n",
                "            \n",
                "            # Critical change for Colab streaming: \n",
                "            # We yield only the NEW chunk. Gradio's audio with streaming=True will append it.\n",
                "            for i, chunk in enumerate(stream):\n",
                "                chunk_np = chunk.cpu().numpy()\n",
                "                chunk_int16 = (chunk_np * 32767).astype(np.int16)\n",
                "                \n",
                "                # Yield JUST the new chunk\n",
                "                yield (SAMPLE_RATE, chunk_int16), f\"Streaming chunk {i+1}...\"\n",
                "            \n",
                "            latency = time.time() - start_time\n",
                "            # Final yield to indicate completion (optional, sometimes sending empty or last status helps)\n",
                "            yield None, (f\"‚úì Streaming complete | \" f\"{latency*1000:.2f} ms total time\")\n",
                "            return\n",
                "\n",
                "        # Non-streaming mode (standard)\n",
                "        start_time = time.perf_counter()\n",
                "\n",
                "        audio = model.infer(\n",
                "            text,\n",
                "            temperature=temperature,\n",
                "            top_p=top_p,\n",
                "            repetition_penalty=repetition_penalty,\n",
                "        )\n",
                "\n",
                "        gen_time = time.perf_counter() - start_time\n",
                "\n",
                "        audio_np = audio.cpu().numpy()\n",
                "        audio_int16 = (audio_np * 32767).astype(np.int16)\n",
                "\n",
                "        audio_seconds = len(audio_np) / SAMPLE_RATE\n",
                "        rtf = audio_seconds / gen_time if gen_time > 0 else float(\"inf\")\n",
                "\n",
                "        status = (\n",
                "            f\"‚úì Generated {audio_seconds:.2f} s audio | \"\n",
                "            f\"Generation time: {gen_time:.3f} s \"\n",
                "            f\"({rtf:.2f}x realtime)\"\n",
                "        )\n",
                "\n",
                "        yield (SAMPLE_RATE, audio_int16), status\n",
                "        return\n",
                "\n",
                "    except Exception as e:\n",
                "        yield None, f\"‚úó Error: {str(e)}\"\n",
                "\n",
                "\n",
                "# Create Gradio interface\n",
                "with gr.Blocks(title=\"Soprano TTS\") as demo:\n",
                "    gr.Markdown(\n",
                "        f\"\"\"# üó£Ô∏è Soprano TTS (GPU Accelerated)\n",
                "        \n",
                "        Running on **{device.upper()}** using **{backend}** backend.\n",
                "        \"\"\"\n",
                "    )\n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=2):\n",
                "            text_input = gr.Textbox(\n",
                "                label=\"Text to Synthesize\",\n",
                "                placeholder=\"Enter text here...\",\n",
                "                value=\"Soprano is an extremely lightweight text to speech model designed to produce highly realistic speech at unprecedented speed.\",\n",
                "                lines=5,\n",
                "                max_lines=10,\n",
                "            )\n",
                "            \n",
                "            streaming = gr.Checkbox(\n",
                "                label=\"Stream Audio\",\n",
                "                value=False,\n",
                "                info=\"Enable streaming generation (updates audio player in real-time)\"\n",
                "            )\n",
                "            \n",
                "            with gr.Accordion(\"Advanced Settings\", open=False):\n",
                "                temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.0, step=0.05, label=\"Temperature\")\n",
                "                top_p = gr.Slider(minimum=0.5, maximum=1.0, value=0.95, step=0.05, label=\"Top P\")\n",
                "                repetition_penalty = gr.Slider(minimum=1.0, maximum=2.0, value=1.2, step=0.1, label=\"Repetition Penalty\")\n",
                "                chunk_size = gr.Slider(minimum=1, maximum=10, value=1, step=1, precision=0, label=\"Chunk Size (Streaming only)\")\n",
                "\n",
                "            generate_btn = gr.Button(\"Generate Speech\", variant=\"primary\", size=\"lg\")\n",
                "        with gr.Column(scale=1):\n",
                "            # streaming=True enables chunk-based output which Gradio queues/appends automatically\n",
                "            audio_output = gr.Audio(\n",
                "                label=\"Generated Speech\", \n",
                "                type=\"numpy\", \n",
                "                autoplay=True, \n",
                "                streaming=True\n",
                "            )\n",
                "            status_output = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
                "\n",
                "    generate_btn.click(\n",
                "        fn=generate_speech,\n",
                "        inputs=[text_input, temperature, top_p, repetition_penalty, chunk_size, streaming],\n",
                "        outputs=[audio_output, status_output],\n",
                "    )\n",
                "\n",
                "print(\"Starting Gradio interface... Please click the public link below.\")\n",
                "demo.launch(share=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
